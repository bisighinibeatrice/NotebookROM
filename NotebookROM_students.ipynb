{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQX1pGqVyeft"
      },
      "source": [
        "## **Case study: Predict stent deployed configuration in real time**\n",
        "\n",
        "In this class, we will build a data-driven reduced order model (ROM) of stent deployment capable of predicting in **real time** the stent deployed configuration given a certain vessel geometry."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLvSGQjjyjCa"
      },
      "source": [
        "### **What are stents?**\n",
        "Stents are small, tube-like devices used to hold open narrowed or blocked arteries or other blood vessels in the body. They are often made of metal or plastic mesh and are commonly used in procedures like angioplasty to restore blood flow in cases of coronary artery disease. Some stents are coated with medication to prevent the artery from narrowing again, while others are simple mechanical supports.\n",
        "\n",
        "There exist two main types of stents. **Balloon-expandable stents** are made from materials like stainless steel and are deployed by inflating a balloon inside the stent, which forces it to expand. They provide high radial force and are ideal for precise placement, such as in coronary arteries. In contrast, **self-expandable stents** are made from shape-memory materials like nitinol. They expand on their own after being released from a catheter and are more flexible, making them suitable for areas that experience movement or compression, like peripheral arteries.\n",
        "\n",
        "In this class, we will focus on the self-expandable stents.\n",
        "\n",
        "![Balloon-expandable stent](https://i.imgur.com/dWsdVgo.jpeg)\n",
        "![Self-expandable stent](https://i.imgur.com/3d7OVob.jpeg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ejSuKhUyocA"
      },
      "source": [
        "### **Why we need a ROM of stents?**\n",
        "Examples of why a ROM for stent deployment is needed:\n",
        "*  **Optimization Study:** A ROM enables running multiple simulations with varying geometrical or material properties of the stent to find the combination that maximizes specific performance metrics, such as radial force, flexibility, and fatigue resistance.\n",
        "*   **Planning Software:** By using a ROM, clinicians can compare different treatment options before actual deployment, evaluating various devices, sizes, and deployment sites to choose the most suitable option for each patient.\n",
        "*   **Intraoperative Support:** ROM can help visualize the deployed configuration overlaid on fluoroscopy images in real-time during the intervention, assisting surgeons with precise positioning and adjustments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWsXc18IuGrO"
      },
      "source": [
        "### **What is the focus of this class?**\n",
        "In this class, we will focus on building a ROM of the deployed configuration of a stent given a set of parameters describing the vessel geometry. As this is an introductory class, we will work on an idealised vessel geometry. Our parameters will represent the curvature and radius of this vessel as long as the clinical decision on where to position the stent.\n",
        "\n",
        "![Parametric vessel](https://i.imgur.com/tccLgJy.png)\n",
        "\n",
        "Eventually, this model could be translated to a realistic case with patient-specific geometries using a geometrical parameters or statistical parameters, computed by building a statistical shape model (see class 18/12/24).\n",
        "\n",
        "![Patient-specific vessel](https://i.imgur.com/dql3B00.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lt8uMLJyuMR"
      },
      "source": [
        "### **How is the class structured?**\n",
        "1. Mathematical definition of the problem\n",
        "2. Sampling\n",
        "3. Building the database\n",
        "4. Building and testing the ROM\n",
        "\n",
        "![Schema](https://i.imgur.com/cjbySFE.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3W_DBcFywLs"
      },
      "source": [
        "### **Reference**\n",
        "About stents:\n",
        "\n",
        "*   https://www.sciencedirect.com/science/article/pii/S1350453310002420\n",
        "*   https://www.sciencedirect.com/science/article/pii/S0021929012003454\n",
        "*   https://www.sciencedirect.com/science/article/pii/S0021929012001789via%3Dihub\n",
        "\n",
        "Review on stents:\n",
        "*   https://www.sciencedirect.com/science/article/pii/S2666522023000175\n",
        "*   https://onlinelibrary.wiley.com/doi/full/10.1002/cnm.3529\n",
        "\n",
        "About reduced order modelling:\n",
        "\n",
        "* https://hal.science/hal-01007232/document\n",
        "* https://amses-journal.springeropen.com/articles/10.1186/s40323-015-0049-1\n",
        "* https://www.degruyter.com/document/doi/10.1515/9783110499001-008/html\n",
        "\n",
        "\n",
        "About this class:\n",
        "* https://www.frontiersin.org/journals/physiology/articles/10.3389/fphys.2023.1148540/full\n",
        "* https://www.sciencedirect.com/science/article/pii/S0045782518303487?casa_token=m-QUUFRG63IAAAAA:bDQR4oqhKto5bPIMnnyrbkdYk88o61SO0xvOC6Rc_8P8O6cvFljDK479s6fRatoobbGr2B2msA\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmOMYsShkW5D"
      },
      "source": [
        "## **Setting up the environment**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Kll6SyVvVnZB"
      },
      "outputs": [],
      "source": [
        "# Setting up the environment\n",
        "import os\n",
        "os.chdir('/content')\n",
        "\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "# Check if the folder 'NotebookROM' exists before attempting to delete it to reinstall it\n",
        "if os.path.exists('NotebookROM'):\n",
        "    # Remove the folder and all its contents\n",
        "    shutil.rmtree('NotebookROM')\n",
        "\n",
        "# Pull the repo with data and install packages for the notebook if we're in Google Colab\n",
        "import os\n",
        "try:\n",
        "  import google.colab # Check if we're in Google Colab\n",
        "  !sudo apt install libgl1-mesa-glx xvfb\n",
        "  !pip install pyvista[jupyter] -qq\n",
        "  current_directory = os.path.basename(os.getcwd())\n",
        "  # If we're not in the right directory or the directory doesn't exist, clone the repo\n",
        "  if not (os.path.isdir('NotebookROM') or current_directory == 'NotebookROM'):\n",
        "    !git clone https://github.com/bisighinibeatrice/NotebookROM.git\n",
        "  if os.path.isdir('NotebookROM'):\n",
        "    %cd NotebookROM\n",
        "except:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fJ24c7y-6po3"
      },
      "outputs": [],
      "source": [
        "# Import the functions\n",
        "from importlib import reload\n",
        "import utils\n",
        "utils = reload(utils) # Reload the module in case it has changed\n",
        "import numpy as np\n",
        "from ipywidgets import interactive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5AVPRH4yXcLl"
      },
      "source": [
        "## **1. Mathematical and physical definition of the problem**\n",
        "\n",
        "The problem we are modelling consists of two entities: the stent and the vessel. A visualisation tool (`Visualizer`) has been implemented to visualize stents and vessels. Here are the documentation and examples of how to use it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "f5iuWhdVaAMn"
      },
      "outputs": [],
      "source": [
        "help(utils.Visualizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wkmffqebmsR"
      },
      "source": [
        "The **stent** we are modeling is a braided design, consisting of 24 interlaced wires with a relaxed radius of 2.7 mm and a length of 12 mm. It is represented using **beam elements**. The mesh data, including node positions and connectivity, is saved in the `Input` folder."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPLAyMhQYyid"
      },
      "source": [
        "> You can visualize the stent using the function `add_stent_from_file()` provided by the `Visualizer` object by passing in input the position and connectivity files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BROL-r1UbmJ5"
      },
      "outputs": [],
      "source": [
        "# Create a visualizer object and add a stent to it\n",
        "viz = utils.Visualizer()\n",
        "viz.add_stent_from_file(\"Input/pos_stent.txt\", \"Input/conn_stent.txt\")\n",
        "viz.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4MPR5TNgoPg"
      },
      "source": [
        "**EXERCISE**: How many nodes compose the stent? How many elements?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "TqMoE-kognol"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kui1_avRcR9j"
      },
      "source": [
        "The **vessel** geometry is represented through a parameterized model, where the parameters serve as inputs for our ROM. The vessel's shape is defined by its **radius** and its **centerline**. With fixed starting and ending points, the centerline is parameterized using a third point, referred to as the control point, which is adjusted within the x-z plane.\n",
        "\n",
        "To create a smooth curve that connects these points, we employ Bernstein basis polynomials to weight the contributions of the three points —start point, control point, and endpoint. This approach effectively blends the points, resulting in a continuous and visually appealing path. This technique is widely used in computer graphics and computational geometry for generating smooth paths and shapes. The `spline_interpolation()` function of the the `Visualizer` object implements this process, computing an interpolated point along a cubic Bézier curve defined by the three points.\n",
        "\n",
        "An additional parameter is considered in our analysis: the **deploy site** of the stent, which indicates its position at the beginning of deployment. This parameter ranges from 0 to 1, where 0 corresponds to the stent being positioned at the start of the vessel (proximal position) and 1 indicates that the stent is located at the end of the vessel (distal position).\n",
        "\n",
        "![Parametric vessel](https://i.imgur.com/dx4kMDu.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPgEr6_tzlEc"
      },
      "source": [
        "> You can visualize a generic vessel using the functions provided by the `Visualizer` object. The centerline is shown in black, the control point in red, and the deploy site in green. The default values provide a straight vessel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "sGhVQ8YJcRam"
      },
      "outputs": [],
      "source": [
        "# Create a visualizer object and add a vessel to it\n",
        "viz = utils.Visualizer()\n",
        "viz.add_vessel()\n",
        "viz.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIjlSdCpLIbD"
      },
      "source": [
        "By changing the input values, we can update the shape of the vessel thought the function `update_vessel()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nI_RLcFdvoXQ"
      },
      "outputs": [],
      "source": [
        "# Update the control point and radius\n",
        "control_point = np.array([60, 0, 25], dtype=np.float64)\n",
        "vessel_radius = 2\n",
        "deploy_site = 0.5\n",
        "\n",
        "viz.update_vessel(control_point, vessel_radius, deploy_site)\n",
        "viz.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5poWZRCLxlW"
      },
      "source": [
        "Through an interactive tool, we can play with the shape of the vessel by changing the parameters value using 4 sliders."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Pq-kgzlbceC7"
      },
      "outputs": [],
      "source": [
        "# Interactively play with the parameters of the vessel\n",
        "viz = utils.Visualizer()\n",
        "viz.add_vessel()\n",
        "\n",
        "def update_vessel(x, z, r, s):\n",
        "    control_point = np.array([x, 0, z], dtype=np.float64)\n",
        "    viz.update_vessel(control_point, r, s)\n",
        "    viz.show()\n",
        "\n",
        "w = interactive(update_vessel, x=(-50, 50, 1), z=(0, 50, 1), r=(1, 20, 1), s=(0, 1, 0.1))\n",
        "for c in w.children:\n",
        "    c.continuous_update = False\n",
        "display(w)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztqqLpCK00XI"
      },
      "source": [
        "## **2. Sampling**\n",
        "\n",
        "In reduced order modelling, the sampling step is crucial for generating a representative set of data points that capture the variability of the system.The Latin Hypercube Sampling (LHS) technique is commonly used for this purpose. LHS is a statistical method that divides the parameter space into equally probable intervals and ensures that samples are distributed uniformly across the entire space. This technique reduces the risk of clustering and improves the efficiency of sampling compared to simple random sampling, making it ideal for high-dimensional problems. It helps in covering the parameter space more thoroughly with fewer samples, which enhances the accuracy of the ROM while minimizing computational costs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKGHaMdLlmS0"
      },
      "source": [
        "This is a Python example of the use of LHS for sampling two parameters (`num_dimensions`). Plot the distribution of the points for different number of samples (`num_samples`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i3EySgbG1YFo"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.stats import qmc\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define number of parameters\n",
        "num_dimensions = 2\n",
        "\n",
        "# Define sample size\n",
        "num_samples = 100\n",
        "\n",
        "# Create a Latin Hypercube Sampler\n",
        "lhs_sampler = qmc.LatinHypercube(d=num_dimensions)\n",
        "lhs_sample = lhs_sampler.random(n=num_samples)\n",
        "\n",
        "# Plot the results\n",
        "plt.scatter(lhs_sample[:, 0], lhs_sample[:, 1], c='red', label='Samples')\n",
        "plt.title('2D Latin Hypercube Sampling')\n",
        "plt.xlabel('Parameter 1 (Range: -5 to 5)')\n",
        "plt.ylabel('Parameter 2 (Range: -4 to 8)')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-K_k3v4Qlwbp"
      },
      "source": [
        "The LHS provides values between 0 and 1. How can we change the ranges in which the parameters are varying? We need to rescale the data.\n",
        "\n",
        "To scale the data from the unit interval [0,1] to a specified range [a,b], we can use this equation:\n",
        "\n",
        "$x_{\\text{scaled}} = a + (b - a) \\cdot x_{\\text{sample}}$\n",
        "\n",
        "where $x_{\\text{scaled}}$ is the scaled value, $x_{\\text{sample}}$ is the original LHS sample in the range [0,1], $a$ is the lower bound of the target range, and $b$ is the upper bound of the target range."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWV917U8ZYXj"
      },
      "source": [
        "Implement this equation to rescale the data and plot the scaled results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kowjpuO0lwHU"
      },
      "outputs": [],
      "source": [
        "# Parameters bounds\n",
        "param1_bounds = (-5.0, 5.0)  # Range for Parameter 1\n",
        "param2_bounds = (-4.0, 8.0)  # Range for Parameter 2\n",
        "\n",
        "# Updated bounds for the two parameters\n",
        "bounds = [param1_bounds, param2_bounds]\n",
        "\n",
        "# Scale the LHS sample to the given bounds\n",
        "scaled_lhs_sample = np.empty_like(lhs_sample)\n",
        "for i, (lower, upper) in enumerate(bounds):\n",
        "    scaled_lhs_sample[:, i] = lower + lhs_sample[:, i] * (upper - lower)\n",
        "\n",
        "# Plot the results\n",
        "plt.scatter(scaled_lhs_sample[:, 0], scaled_lhs_sample[:, 1], c='red', label='Samples')\n",
        "plt.title('2D Latin Hypercube Sampling')\n",
        "plt.xlabel('Parameter 1 (Range: -5 to 5)')\n",
        "plt.ylabel('Parameter 2 (Range: -4 to 8)')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H04VNLPo1hpg"
      },
      "source": [
        "Let's go back to our original problem of interest. We have 4 parameters (vessel radius, control point x and z coordinates, and deploy site), which are varying in these ranges:\n",
        "*   `vessel_radius_bounds = (1.0, 2.5)`\n",
        "*   `control_point_x_bounds = (0.0, 50.0)`\n",
        "*   `control_point_z_bounds = (0.0, 50.0)`\n",
        "*   `deploy_site_bounds = (0.01, 1.0)`\n",
        "\n",
        "**EXERCISE:**  Change the LHS code to compute `num_samples = 5` with LHS (considering the ranges indicated above) and visualise them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dru5laci1hKP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0UsyCpxMGKx"
      },
      "source": [
        "## **3. Building the database**\n",
        "\n",
        "To create the ROM, we considered four scenarios with an increasing number of cases: 100, 200, 500, and 1000. Using the LHS method, we determined the parameter values and generated the corresponding geometries using a dedicated Python routine. For each of these geometries, we then ran a simulation of stent deployment using an in-house, finite-element software. This simulation consists in computing, starting from the stent configuration previously shown and the deploy site defined by the LHS, the deployed configuration of the stent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jkvOcPs3spm"
      },
      "source": [
        "These simulations are performed in three steps:\n",
        "\n",
        "1. First the stent is crimped to fit within the vessel. ![Crimping](https://i.imgur.com/xSBHAJf.gif)\n",
        "\n",
        "2. Then, positioned along the centerline of the vessel. ![Positioning](https://i.imgur.com/ChGbkJU.gif)\n",
        "\n",
        "3. And finally, deployed. ![Deployment](https://i.imgur.com/hsWHPZJ.gif)\n",
        "\n",
        "As this topic will not be covered in the current class, additional information on finite-element simulations of stent deployment can be found in the literature provided as reference material."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYSMevX1qbFP"
      },
      "source": [
        "For the following step, the data corresponding to the input (`parameters_$N.txt`) and output (`simulations_$N.txt`) of these simulations was saved in the `Input` folder. The output consists in the displacements of the stent nodes at the end of the simulation. We can visualise a sample case using the `Visualizer` tool."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rXpnKZiNy60q"
      },
      "outputs": [],
      "source": [
        "# Specify the database size and the case\n",
        "N = 100\n",
        "line_number = 1  # Line number to be read from the displacements file\n",
        "\n",
        "# Specify the names of the input files\n",
        "position_file = \"Input/pos_stent.txt\"  # Position data file (positions for the stent)\n",
        "connectivity_file = \"Input/conn_stent.txt\"  # Connectivity data file\n",
        "displacement_file = f\"Input/simulations_{N}.txt\"  # Displacement data file (displacement vector for simulations)\n",
        "\n",
        "# Read the position matrix from position_file\n",
        "position = np.loadtxt(position_file)\n",
        "connectivity = np.loadtxt(connectivity_file).astype(np.int32)\n",
        "\n",
        "# Open displacement_file to read the specified line (line_number)\n",
        "with open(displacement_file, 'r') as displacement_file_handle:\n",
        "    # Read all lines into a list\n",
        "    lines = displacement_file_handle.readlines()\n",
        "    if line_number <= len(lines):  # Ensure that line_number is within the number of available lines\n",
        "        displacement_vector = list(map(float, lines[line_number - 1].strip().split()))  # Read and convert the line to floats\n",
        "    else:\n",
        "        raise ValueError(f\"Line {line_number} does not exist in the file.\")\n",
        "\n",
        "# Reshape the displacement vector into a matrix with 3 columns\n",
        "displacement = np.array(displacement_vector).reshape(-1, 3)\n",
        "\n",
        "# Add the position matrix and displacement matrix element-wise to get the final position matrix\n",
        "final_position = position + displacement\n",
        "\n",
        "# Create a visualizer object and add a stent to it\n",
        "visualizer = utils.Visualizer()\n",
        "visualizer.add_stent(final_position, connectivity)\n",
        "\n",
        "# Update the control point and radius based on parameters from a different file\n",
        "parameter_file = f\"Input/parameters_{N}.txt\"\n",
        "\n",
        "with open(parameter_file, 'r') as parameter_file_handle:\n",
        "    # Read all lines from the parameter file\n",
        "    lines = parameter_file_handle.readlines()\n",
        "    if line_number <= len(lines):  # Ensure that line_number is within the number of available lines\n",
        "        parameter_vector = list(map(float, lines[line_number - 1].strip().split()))  # Read and convert the line to floats\n",
        "    else:\n",
        "        raise ValueError(f\"Line {line_number} does not exist in the file.\")\n",
        "\n",
        "# Define the control point, vessel radius, and deployment site from the parameters\n",
        "control_point = np.array([parameter_vector[1], 0, parameter_vector[2]], dtype=np.float64)\n",
        "vessel_radius = parameter_vector[0]\n",
        "deployment_site = parameter_vector[3]\n",
        "\n",
        "# Update the vessel display with the new control point and radius\n",
        "visualizer.add_vessel(control_point, vessel_radius, deployment_site)\n",
        "visualizer.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLPZ1L9JTAC6"
      },
      "source": [
        "## **4. Building and testing the ROM**\n",
        "\n",
        "Machine learning is a key technique used in building ROMs for complex simulations. In this class, we will focus on regression models. **Regression** involves fitting a mathematical function to approximate the relationship between input parameters (e.g., geometry, material properties) and output variables (e.g., stress, displacement). There are many machine learning techniques, ranging from simpler models like linear regression to more complex models capable of capturing nonlinear relationships, such as neural networks.\n",
        "\n",
        "In this class, we will use Gaussian process regression (GPR), as it is well-suited for small training datasets and has previously been applied to nonlinear structural problems."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xNCfzwz-UJp_"
      },
      "outputs": [],
      "source": [
        "# Import Libraries\n",
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from sklearn.gaussian_process.kernels import RBF, WhiteKernel\n",
        "from sklearn.gaussian_process.kernels import ConstantKernel as C\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "# Load parameters and simulations from text files\n",
        "def load_data(parameters_file, simulations_file):\n",
        "    parameters = np.loadtxt(parameters_file)\n",
        "    simulations = np.loadtxt(simulations_file)\n",
        "    return parameters, simulations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3N5nute05x5"
      },
      "source": [
        "Initially, we will start from the smallest database (number of cases: `N = 100`) and use the data in its raw form as input for the machine learning model. The data has to be split between training set (used to train the model) and testing set (used to test the model, not used during the training)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "MH243fWMd6xf"
      },
      "outputs": [],
      "source": [
        "# Load the data\n",
        "N = 100\n",
        "parameters_file = f\"Input/parameters_{N}.txt\"  # Replace with your actual file name\n",
        "simulations_file = f\"Input/simulations_{N}.txt\"  # Replace with your actual file name\n",
        "parameters, simulations = load_data(parameters_file, simulations_file)\n",
        "indices = np.arange(len(parameters))\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test, idx_train, idx_test = train_test_split(parameters, simulations, indices, test_size=0.25, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-ImtIhQ1GHM"
      },
      "source": [
        "In Gaussian Process Regression, the choice of kernel function defines the behavior of the model. It defines how similar points influence each other in predictions. Here, we’ll use a common choice: a combination of the RBF kernel and a constant kernel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v_U-w1YF1H0E"
      },
      "outputs": [],
      "source": [
        "# Build the Kriging model on the data: define a kernel for the Gaussian Process\n",
        "initial_kernel = C(1.0, (1e0, 1e3)) * RBF(1.0, (1e-2, 1e2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKAqfKVb1TXM"
      },
      "source": [
        "Then, we can create the GPR model and fit the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FtTB73L21S2F"
      },
      "outputs": [],
      "source": [
        "# Create the GaussianProcessRegressor with the defined kernel\n",
        "gp = GaussianProcessRegressor(kernel=initial_kernel, n_restarts_optimizer=10, random_state=42)\n",
        "\n",
        "# Fit the Gaussian Process model to the reduced data\n",
        "gp.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8UgJDaEJ1aLM"
      },
      "source": [
        "Now that it has been trained, we can use it to make prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "itMStIJn2gp5"
      },
      "outputs": [],
      "source": [
        "# Predict using the Kriging model on the test data\n",
        "y_pred = gp.predict(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDzDlcSWI9GR"
      },
      "source": [
        "The model has been successfully trained and now we need to **evaluate** its performance. For a regression model, we can assess its performance using metrics such as:\n",
        "*   **Mean absolute error** (**MAE**): Measures the average magnitude of errors in absolute terms (**Pros:** Intuitive to understand and interpret. **Cons:** Does not penalize large errors heavily).\n",
        "*   **Mean square error** (**MSE**): Measures the average squared difference between actual and predicted values (**Pros:** Penalizes larger errors more than MAE. **Cons:**  Sensitive to outliers).\n",
        "*   **Root Mean Squared Error** (**RMSE**): The square root of MSE, expressed in the same units as the original data (**Pros:** Easy to interpret due to its unit consistency. **Cons:** Like MSE, it is sensitive to outliers).\n",
        "\n",
        "The expressions for the error metrics are:\n",
        "\n",
        "*   **Mean Squared Error (MSE):** $\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$\n",
        "\n",
        "*    **Mean Absolute Error (MAE):** $\\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i|$\n",
        "\n",
        "*    **Root Mean Squared Error (RMSE):** $\\text{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}$\n",
        "\n",
        "with:\n",
        "\n",
        "*    $y_i$: The true value or the actual value from the dataset.\n",
        "*    $\\hat{y}_i$: The predicted value for the radius obtained from the reduced-order model.\n",
        "*   $n$: The total number of data points or samples in the dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MY88wF1pnsEP"
      },
      "outputs": [],
      "source": [
        "# Calculate MSE, MAE, and RMSE for each case, including standard deviation\n",
        "mae_vec = []\n",
        "rmse_vec = []\n",
        "mse_vec = []\n",
        "\n",
        "for i in range(y_test.shape[0]):\n",
        "    mae = np.mean(np.abs(y_test[i, :] - y_pred[i, :]))\n",
        "    mse = np.mean((y_test[i, :] - y_pred[i, :]) ** 2)\n",
        "    rmse = np.sqrt(mse)\n",
        "\n",
        "    mae_vec.append(mae)\n",
        "    mse_vec.append(mse)\n",
        "    rmse_vec.append(rmse)\n",
        "\n",
        "# Calculate the mean and standard deviation for each error metric\n",
        "mean_mae = np.mean(mae_vec)\n",
        "std_mae = np.std(mae_vec)\n",
        "\n",
        "mean_mse = np.mean(mse_vec)\n",
        "std_mse = np.std(mse_vec)\n",
        "\n",
        "mean_rmse = np.mean(rmse_vec)\n",
        "std_rmse = np.std(rmse_vec)\n",
        "\n",
        "# Print the results\n",
        "print(f\"MAE: {mean_mae} mm (Standard Deviation: {std_mae} mm)\")\n",
        "print(f\"MSE: {mean_mse} (Standard Deviation: {std_mse})\")\n",
        "print(f\"RMSE: {mean_rmse} mm (Standard Deviation: {std_rmse} mm)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6AOejgLOsvu"
      },
      "source": [
        "Absolute errors, expressed in millimeters, are straightforward to interpret. However, their significance becomes even clearer when contextualized with a relative error. A relative error, expressed as a percentage, relates the absolute error (e.g., RMSE) to a physical reference, such as the average radius of the vessels in our database. This approach provides a normalized measure of error, allowing for better understanding and comparison across different scales.\n",
        "\n",
        "Below is the code to compute the average radius from a file where the first element of each row represents the radius of a vessel. This average radius will serve as the reference for calculating the relative error:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ej5Hc0WHmj6m"
      },
      "outputs": [],
      "source": [
        "# Specify the file path\n",
        "parameter_file = f\"Input/parameters_{N}.txt\"\n",
        "\n",
        "# Initialize a list to store radii\n",
        "radii = []\n",
        "\n",
        "# Open and read the file\n",
        "with open(parameter_file, 'r') as file:\n",
        "    for line in file:\n",
        "        # Split the line into elements and take the first one as the radius\n",
        "        radius = float(line.split()[0])  # Assumes the file is space-separated\n",
        "        radii.append(radius)\n",
        "\n",
        "# Compute the average radius\n",
        "average_radius = sum(radii) / len(radii) if radii else 0\n",
        "\n",
        "print(f\"Average Radius: {average_radius}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6p-dccLnovV"
      },
      "source": [
        "With the average_radius value calculated, you can now compute the relative error as:\n",
        "\n",
        "$\\text{Relative Error (%)} = \\left( \\frac{\\text{RMSE}}{\\text{Average Radius}} \\right) \\times 100$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l_oDmyKTOqpK"
      },
      "outputs": [],
      "source": [
        "# Compute the relative error based on the mean radius\n",
        "relative_error = (mean_rmse / average_radius) * 100\n",
        "print(f\"Relative Error: {relative_error:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkFfkNhaeObm"
      },
      "source": [
        "The errors are quite high, which means that the machine learning model is struggling to establish a mapping between the input and output when using the raw data.\n",
        "\n",
        "This could be due to the large variation between the input and output data. To evaluate this, we can compute statistical measures like the variance or standard deviation for both the input and output datasets. These metrics will help us quantify the spread or dispersion in the data, providing insight into whether significant differences in scale or range between the input and output are affecting the model's ability to learn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RNQDRx7lqdcM"
      },
      "outputs": [],
      "source": [
        "# Calculate standard deviation for input and output\n",
        "input_std_dev = np.std(parameters, axis=0)\n",
        "output_std_dev = np.std(simulations, axis=0)\n",
        "\n",
        "print(input_std_dev)\n",
        "print(output_std_dev)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8eKbn40os_2"
      },
      "source": [
        "To improve performance, we can **standardize** the data, which involves transforming the features to have a mean of zero and a standard deviation of one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Uwm_J-WlaXx"
      },
      "outputs": [],
      "source": [
        "# Standardize both training and test sets for the output\n",
        "scaler_y = StandardScaler()\n",
        "y_train_scaled = scaler_y.fit_transform(y_train)\n",
        "y_test_scaled = scaler_y.transform(y_test)\n",
        "\n",
        "# Standardize both training and test sets for the input\n",
        "scaler_x = StandardScaler()\n",
        "X_train_scaled = scaler_x.fit_transform(X_train)\n",
        "X_test_scaled = scaler_x.transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K26HNsNb3Muu"
      },
      "source": [
        "Let's visualise the change in variance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "doni-hmf20Fi"
      },
      "outputs": [],
      "source": [
        "# Compute the mean values for the first 10 output variables before and after scaling\n",
        "mean_before = np.mean(y_train[:, :10], axis=0)\n",
        "mean_after = np.mean(y_train_scaled[:, :10], axis=0)\n",
        "\n",
        "# Compute the standard deviations for the first 10 output variables before and after scaling\n",
        "std_before = np.std(y_train[:, :10], axis=0)\n",
        "std_after = np.std(y_train_scaled[:, :10], axis=0)\n",
        "\n",
        "# Plot the means with error bars representing standard deviation before scaling\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.errorbar(range(1, 11), mean_before, yerr=std_before, fmt='o', label='Before Scaling', color='blue', capsize=5)\n",
        "plt.title('Mean and Standard Deviation for the First 10 Output Variables (Before Scaling)')\n",
        "plt.xlabel('Output Variable')\n",
        "plt.ylabel('Mean Value')\n",
        "plt.xticks(range(1, 11))\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Plot the means with error bars representing standard deviation after scaling\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.errorbar(range(1, 11), mean_after, yerr=std_after, fmt='o', label='After Scaling', color='orange', capsize=5)\n",
        "plt.title('Mean and Standard Deviation for the First 10 Output Variables (After Scaling)')\n",
        "plt.xlabel('Output Variable')\n",
        "plt.ylabel('Mean Value')\n",
        "plt.xticks(range(1, 11))\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7_sGNy53Hd6"
      },
      "source": [
        "**EXERCIZE:**  Train again the machine learning model but with the scaled data now."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w9mEbiJR2Bmv"
      },
      "outputs": [],
      "source": [
        "# Build the Kriging model on the data: define a kernel for the Gaussian Process\n",
        "initial_kernel = C(1.0, (1e-1, 1e3)) * RBF(1.0, (1e-2, 1e2))\n",
        "\n",
        "# Create the GaussianProcessRegressor with the defined kernel\n",
        "gp = GaussianProcessRegressor(kernel=initial_kernel, n_restarts_optimizer=10, random_state=42)\n",
        "\n",
        "# Fit the model\n",
        "gp.fit(, )\n",
        "\n",
        "# Predict using the Kriging model on the test data\n",
        "y_pred_scaled = gp.predict()\n",
        "\n",
        "# Inverse transform to reconstruct the original output\n",
        "y_pred = scaler_y.inverse_transform()\n",
        "\n",
        "# Calculate RMSE for each case\n",
        "rmse_vec = []\n",
        "for i in range(y_test.shape[0]):\n",
        "    rmse = np.sqrt(mean_squared_error(y_test[i, :], y_pred[i, :]))\n",
        "    rmse_vec.append(rmse)\n",
        "\n",
        "print(f\"RMSE: {np.mean(rmse_vec)} mm\")\n",
        "\n",
        "# Compute the relative error based on the mean radius\n",
        "relative_error = (np.mean(rmse_vec) / average_radius) * 100\n",
        "print(f\"Relative Error: {relative_error:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5E_ghUmRlpew"
      },
      "source": [
        "The accuracy of the machine learning model is much better! But the code still takes a lot of time to run. Let's quantify this time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "89ujL_4oryvp"
      },
      "outputs": [],
      "source": [
        "# Measure the time taken to fit the model\n",
        "start_time = time.time()\n",
        "gp.fit(X_train_scaled, y_train_scaled)\n",
        "end_time = time.time()\n",
        "\n",
        "# Compute the computational time\n",
        "fit_time = end_time - start_time\n",
        "print(f\"Time taken to fit the Gaussian Process model: {fit_time:.4f} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Ecg8bpfrrq8"
      },
      "source": [
        "This long training time may be due to the complexity of learning all of these outputs simultaneously.\n",
        "\n",
        "**EXERCISE:** How many output is our system composed of?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1z33VTWDq_D9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gs5LTCqlmA4b"
      },
      "source": [
        "To reduce the training time, we can **compress** this output, and one effective approach is to use the Principal Component Analysis (PCA). This technique reduces the dimensionality of the data by transforming it into a set of uncorrelated variables, known as principal components, which capture the most significant variance in the output while minimizing information loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VY9W9wINm4so"
      },
      "outputs": [],
      "source": [
        "# Create the PCA on the output (simulations)\n",
        "n_components = 25\n",
        "pca_simulations = PCA(n_components=n_components)\n",
        "\n",
        "# Fit and transform the output\n",
        "y_train_scaled_reduced = pca_simulations.fit_transform(y_train_scaled)\n",
        "\n",
        "# Plot the cumulative explained variance\n",
        "explained_variance = np.cumsum(pca_simulations.explained_variance_ratio_)\n",
        "\n",
        "# Create the plot\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(range(1, n_components + 1), explained_variance, marker='o', linestyle='--', color='b')\n",
        "plt.xlabel('Number of Components')\n",
        "plt.ylabel('Cumulative Explained Variance')\n",
        "plt.title('PCA Convergence: Cumulative Explained Variance vs. Number of Components')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TU7pfdda8KvU"
      },
      "source": [
        "This plot shows the cumulative explained variance against the number of principal components in a PCA analysis:\n",
        "\n",
        "* **X-axis**: Number of principal components added.\n",
        "* **Y-axis**: Cumulative explained variance—how much of the data’s variance is captured as more components are included.\n",
        "* **Curve**: The curve rises quickly at first, indicating that the initial components capture most of the variance, then levels off, suggesting diminishing returns from additional components.\n",
        "\n",
        "This plot helps identify the \"elbow point,\" where adding more components provides minimal additional explained variance, guiding you to select an optimal number of components for dimensionality reduction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGCMnsYUvER_"
      },
      "source": [
        "This plot helps identify the \"elbow point\", where adding more components provides minimal additional explained variance, guiding you to select an optimal number of components for dimensionality reduction.\n",
        "\n",
        "To compute the \"elbow point\" for PCA, which represents the optimal number of components beyond which additional components contribute less to explaining the variance, we can analyze the cumulative explained variance and look for the \"elbow\" in the plot.\n",
        "\n",
        "**Steps to find the elbow point:**\n",
        "1.   Calculate the cumulative explained variance.\n",
        "2.   Compute the \"rate of change\" (difference) in the explained variance between successive components.\n",
        "3.  The elbow point corresponds to the component where the rate of change starts to decrease sharply, indicating the point beyond which adding more components contributes significantly less to the explained variance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rOgsOAepvD8I"
      },
      "outputs": [],
      "source": [
        "# Compute the rate of change in explained variance\n",
        "rate_of_change = np.diff(explained_variance)\n",
        "\n",
        "# Find the elbow point (the point where the rate of change drops)\n",
        "thresold = 0.001\n",
        "elbow_point = np.argmax(rate_of_change < thresold) + 1  # +1 because np.diff reduces the array size by 1\n",
        "\n",
        "# Create the plot\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(range(1, n_components + 1), explained_variance, marker='o', linestyle='--', color='b')\n",
        "plt.axvline(elbow_point, color='r', linestyle='--', label=f'Elbow Point: {elbow_point}')\n",
        "plt.xlabel('Number of Components')\n",
        "plt.ylabel('Cumulative Explained Variance')\n",
        "plt.title('PCA Convergence: Cumulative Explained Variance vs. Number of Components')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Print the elbow point\n",
        "print(f\"The elbow point is at component number: {elbow_point}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzrjcquh8CzS"
      },
      "source": [
        "**EXERCIZE:**  Train again the machine learning model but with the scaled reuduced data now."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ln3atvRv0OgF"
      },
      "outputs": [],
      "source": [
        "# Build the Kriging model on the data: define a kernel for the Gaussian Process\n",
        "initial_kernel = C(1.0, (1e2, 1e5)) * RBF(1.0, (1e-2, 1e2)) + WhiteKernel(noise_level=1e-2)\n",
        "\n",
        "# Create the GaussianProcessRegressor with the defined kernel\n",
        "gp = GaussianProcessRegressor(kernel=initial_kernel, n_restarts_optimizer=10, random_state=42)\n",
        "\n",
        "# Measure the time taken to fit the model\n",
        "start_time = time.time()\n",
        "gp.fit( , )\n",
        "end_time = time.time()\n",
        "\n",
        "# Compute the computational time\n",
        "fit_time = end_time - start_time\n",
        "print(f\"Time taken to fit the Gaussian Process model: {fit_time:.4f} seconds\")\n",
        "\n",
        "# Predict using the Kriging model on the test data\n",
        "y_pred_scaled_reduced = gp.predict()\n",
        "\n",
        "# Inverse transform to reconstruct the original output\n",
        "y_pred_scaled = pca_simulations.inverse_transform()\n",
        "y_pred = scaler_y.inverse_transform()\n",
        "\n",
        "# Calculate RMSE for each case\n",
        "rmse_vec = []\n",
        "for i in range(y_test.shape[0]):\n",
        "    rmse = np.sqrt(mean_squared_error(y_test[i, :], y_pred[i, :]))\n",
        "    rmse_vec.append(rmse)\n",
        "\n",
        "print(f\"RMSE: {np.mean(rmse_vec)} mm\")\n",
        "\n",
        "# Compute the relative error based on the mean radius\n",
        "relative_error = (np.mean(rmse_vec) / average_radius) * 100\n",
        "print(f\"Relative Error: {relative_error:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d99-QW5eSQ8-"
      },
      "source": [
        "The computational time is improved drastically while maintaing the precision of the previous case! We can now visualise the results of the prediction to better understand the scale of these errors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BJC9TTAB9r1N"
      },
      "outputs": [],
      "source": [
        "# Specify the case\n",
        "N = 100\n",
        "case = 20  # Case number to use from y_pred\n",
        "\n",
        "# Specify the name of the position file\n",
        "position_file = \"Input/pos_stent.txt\"  # Position data file (positions for the stent)\n",
        "connectivity_file = \"Input/conn_stent.txt\"\n",
        "\n",
        "# Read the position matrix from position_file\n",
        "position = np.loadtxt(position_file)\n",
        "connectivity = np.loadtxt(connectivity_file).astype(np.int32)\n",
        "\n",
        "# Ensure that case is within the range of y_pred (predicted displacements)\n",
        "if case <= len(y_pred):  # Assuming y_pred is a list of lists or a 2D array where each row is a displacement vector\n",
        "    displacement_pred_vector = y_pred[case - 1]  # Select the specified case from y_pred (predicted displacement)\n",
        "else:\n",
        "    raise ValueError(f\"Case {case} does not exist in y_pred.\")\n",
        "\n",
        "# Reshape the displacement vector into a matrix with 3 columns\n",
        "displacement_pred = np.array(displacement_pred_vector).reshape(-1, 3)\n",
        "\n",
        "# Add the position matrix and predicted displacement matrix element-wise to get the predicted final position matrix\n",
        "final_position_pred = position + displacement_pred\n",
        "\n",
        "# Ensure that case is within the range of y_test (actual displacements)\n",
        "if case <= len(y_test):  # Assuming y_test is a list of lists or a 2D array where each row is a displacement vector\n",
        "    displacement_actual_vector = y_test[case - 1]  # Select the specified case from y_test (actual displacement)\n",
        "else:\n",
        "    raise ValueError(f\"Case {case} does not exist in y_test.\")\n",
        "\n",
        "# Reshape the displacement vector into a matrix with 3 columns\n",
        "displacement_actual = np.array(displacement_actual_vector).reshape(-1, 3)\n",
        "\n",
        "# Add the position matrix and actual displacement matrix element-wise to get the final position matrix\n",
        "final_position_actual = position + displacement_actual\n",
        "\n",
        "# Create a visualizer object and add both stents to it\n",
        "visualizer = utils.Visualizer()\n",
        "visualizer.add_stent(final_position_pred, connectivity, color=\"green\")  # Predicted stent in green\n",
        "visualizer.add_stent(final_position_actual, connectivity, color=\"red\")  # Actual stent in red\n",
        "\n",
        "# Update the control point and radius based on parameters from a different file\n",
        "parameter_file = f\"Input/parameters_{N}.txt\"\n",
        "\n",
        "with open(parameter_file, 'r') as parameter_file_handle:\n",
        "    # Read all lines from the parameter file\n",
        "    lines = parameter_file_handle.readlines()\n",
        "    if idx_test[case - 1] <= len(lines):  # Ensure that case is within the number of available lines\n",
        "        parameter_vector = list(map(float, lines[idx_test[case - 1]].strip().split()))  # Read and convert the line to floats\n",
        "    else:\n",
        "        raise ValueError(f\"Case {idx_test[case - 1]} does not exist in the parameter file.\")\n",
        "\n",
        "# Define the control point, vessel radius, and deployment site from the parameters\n",
        "control_point = np.array([parameter_vector[1], 0, parameter_vector[2]], dtype=np.float64)\n",
        "vessel_radius = parameter_vector[0]\n",
        "deployment_site = parameter_vector[3]\n",
        "\n",
        "# Update the vessel display with the new control point and radius\n",
        "visualizer.add_vessel(control_point, vessel_radius, deployment_site)\n",
        "visualizer.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**EXERCISE:** Write in a single cell all the passages for building a ROM with scaled and reduced data but now using the largest dataset. Visualise the predicted stent and compare it with the one predicted using the machine learning trained with the smallest dataset."
      ],
      "metadata": {
        "id": "RJi9IBI2S0Lz"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2xJHcOZ6S0ie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxVBSv3OzjHs"
      },
      "source": [
        "# Extra\n",
        "\n",
        "We can now compare the performance of the machine learning model when we use a\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RIjQDJnNzr2N"
      },
      "outputs": [],
      "source": [
        "# Prepare lists to store the errors\n",
        "N_values = [100, 250, 500, 1000]\n",
        "rmse_results = []\n",
        "rmse_std_results = []\n",
        "\n",
        "for N in N_values:\n",
        "\n",
        "    # Example usage\n",
        "    parameters_file = f\"Input/parameters_{N}.txt\"  # Replace with your actual file name\n",
        "    simulations_file = f\"Input/simulations_{N}.txt\"  # Replace with your actual file name\n",
        "\n",
        "    # Load the data\n",
        "    parameters, simulations = load_data(parameters_file, simulations_file)\n",
        "    indices = np.arange(len(parameters))\n",
        "\n",
        "    # Split the data into training and test sets\n",
        "    X_train, X_test, y_train, y_test, idx_train, idx_test = train_test_split(parameters, simulations, indices, test_size=0.25, random_state=42)\n",
        "\n",
        "    # Save the idx_test (indices of the test set) to a text file\n",
        "    idx_test_file = f\"Input/idx_test_{N}.txt\"\n",
        "    np.savetxt(idx_test_file, idx_test, fmt='%d', delimiter=\"\\t\")\n",
        "\n",
        "    # Standardize both training and test sets for the output\n",
        "    scaler_y = StandardScaler()\n",
        "    y_train_scaled = scaler_y.fit_transform(y_train)\n",
        "    y_test_scaled = scaler_y.transform(y_test)\n",
        "\n",
        "    # Standardize both training and test sets for the input\n",
        "    scaler_x = StandardScaler()\n",
        "    X_train_scaled = scaler_x.fit_transform(X_train)\n",
        "    X_test_scaled = scaler_x.transform(X_test)\n",
        "\n",
        "    # Create the PCA on the output (simulations)\n",
        "    n_components = 50\n",
        "    pca_simulations = PCA(n_components=n_components)\n",
        "\n",
        "    # Fit and transform the output\n",
        "    y_train_scaled_reduced = pca_simulations.fit_transform(y_train_scaled)\n",
        "\n",
        "    # Build the Kriging model on the data: define a kernel for the Gaussian Process\n",
        "    initial_kernel = C(1.0, (1e2, 1e5)) * RBF(1.0, (1e-2, 1e2)) + WhiteKernel(noise_level=1e-2)\n",
        "\n",
        "    # Create the GaussianProcessRegressor with the defined kernel\n",
        "    gp = GaussianProcessRegressor(kernel=initial_kernel, n_restarts_optimizer=10, random_state=42)\n",
        "\n",
        "    # Fit the Gaussian Process model to the reduced data\n",
        "    gp.fit(X_train_scaled, y_train_scaled_reduced)\n",
        "\n",
        "    # Predict using the Kriging model on the test data\n",
        "    y_pred_scaled_reduced = gp.predict(X_test_scaled)\n",
        "\n",
        "    # Inverse transform to reconstruct the original output\n",
        "    y_pred_scaled = pca_simulations.inverse_transform(y_pred_scaled_reduced)\n",
        "    y_pred = scaler_y.inverse_transform(y_pred_scaled)\n",
        "\n",
        "    # Save predictions to a text file\n",
        "    prediction_file = f\"Input/predictions_{N}.txt\"\n",
        "    np.savetxt(prediction_file, y_pred, fmt='%.6f', delimiter=\"\\t\")\n",
        "\n",
        "    # Calculate RMSE for each case\n",
        "    rmse_vec = []\n",
        "    for i in range(y_test.shape[0]):\n",
        "        rmse = np.sqrt(mean_squared_error(y_test[i, :], y_pred[i, :]))\n",
        "        rmse_vec.append(rmse)\n",
        "\n",
        "    print(f\"Number of training cases: {N}\")\n",
        "    print(f\"  mean of RMSE: {np.mean(rmse_vec)} mm\")\n",
        "    print(f\"  median of RMSE: {np.median(rmse_vec)} mm \")\n",
        "    print(f\"  sd of RMSE: {np.std(rmse_vec)} mm\")\n",
        "\n",
        "\n",
        "    # Compute the relative error based on the mean radius\n",
        "    relative_error = (np.mean(rmse_vec) / average_radius) * 100\n",
        "    print(f\"  relative error: {relative_error:.2f}%\")\n",
        "\n",
        "    # Store the results\n",
        "    rmse_results.append(np.mean(rmse_vec))\n",
        "    rmse_std_results.append(np.std(rmse_vec))\n",
        "\n",
        "# Plotting MAE and RMSE vs. number of training samples\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.errorbar(N_values, rmse_results, yerr=rmse_std_results, label='RMSE', marker='o', capsize=5)\n",
        "plt.xlabel(\"Number of Training Samples\")\n",
        "plt.ylabel(\"Error\")\n",
        "plt.title(\"Error vs Number of Training Samples\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LazdnmCWNfFn"
      },
      "source": [
        "Finally, let's visualise the stent predited by training the machine learning model with `N = 1000` cases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "XSZVOra2G3Xv"
      },
      "outputs": [],
      "source": [
        "# Specify the database and case\n",
        "N = 500\n",
        "case = 3  # Case number to use from y_pred and y_actual\n",
        "\n",
        "# Specify the name of the position file\n",
        "position_file = \"Input/pos_stent.txt\"  # Position data file (positions for the stent)\n",
        "connectivity_file = \"Input/conn_stent.txt\"\n",
        "\n",
        "# Read the position matrix from position_file\n",
        "position = np.loadtxt(position_file)\n",
        "connectivity = np.loadtxt(connectivity_file).astype(np.int32)\n",
        "\n",
        "# Read the predicted displacements (y_pred) from the prediction file (one line for each case)\n",
        "predictions_file = f\"Input/predictions_{N}.txt\"\n",
        "y_pred = np.loadtxt(predictions_file)\n",
        "\n",
        "# Ensure that case is within the range of y_pred (predicted displacements)\n",
        "if case <= len(y_pred):  # Assuming y_pred is a list of rows where each row is a displacement vector\n",
        "    displacement_pred_vector = y_pred[case - 1]  # Select the specified case (predicted displacement)\n",
        "else:\n",
        "    raise ValueError(f\"Case {case} does not exist in y_pred.\")\n",
        "\n",
        "# Reshape the displacement vector into a matrix with 3 columns\n",
        "displacement_pred = np.array(displacement_pred_vector).reshape(-1, 3)\n",
        "\n",
        "# Add the position matrix and predicted displacement matrix element-wise to get the predicted final position matrix\n",
        "final_position_pred = position + displacement_pred\n",
        "\n",
        "# Read the actual displacements (y_actual) from the simulations file (one line for each case)\n",
        "simulations_file = f\"Input/simulations_{N}.txt\"\n",
        "y_actual = np.loadtxt(simulations_file)\n",
        "\n",
        "# Read idx_test from the previously saved text file\n",
        "idx_test_file = f\"Input/idx_test_{N}.txt\"\n",
        "idx_test = np.loadtxt(idx_test_file, dtype=int)  # Read idx_test values from the file\n",
        "\n",
        "# Ensure that case is within the range of idx_test (test indices)\n",
        "if idx_test[case - 1] <= len(y_actual):  # Ensure the case is within the range of actual displacements\n",
        "    displacement_actual_vector = y_actual[idx_test[case - 1]]  # Select the specified case (actual displacement)\n",
        "else:\n",
        "    raise ValueError(f\"Case {idx_test[case - 1]} does not exist in y_actual.\")\n",
        "\n",
        "# Reshape the displacement vector into a matrix with 3 columns\n",
        "displacement_actual = np.array(displacement_actual_vector).reshape(-1, 3)\n",
        "\n",
        "# Add the position matrix and actual displacement matrix element-wise to get the final position matrix\n",
        "final_position_actual = position + displacement_actual\n",
        "\n",
        "# Create a visualizer object and add both stents to it\n",
        "visualizer = utils.Visualizer()\n",
        "visualizer.add_stent(final_position_pred, connectivity, color=\"green\")  # Predicted stent in green\n",
        "visualizer.add_stent(final_position_actual, connectivity, color=\"red\")  # Actual stent in red\n",
        "\n",
        "# Update the control point and radius based on parameters from a different file\n",
        "parameter_file = f\"Input/parameters_{N}.txt\"\n",
        "\n",
        "with open(parameter_file, 'r') as parameter_file_handle:\n",
        "    # Read all lines from the parameter file\n",
        "    lines = parameter_file_handle.readlines()\n",
        "    if idx_test[case - 1] <= len(lines):  # Ensure that case is within the number of available lines\n",
        "        parameter_vector = list(map(float, lines[idx_test[case - 1]].strip().split()))  # Read and convert the line to floats\n",
        "    else:\n",
        "       raise ValueError(f\"Case {idx_test[case - 1]} does not exist in the parameter file.\")\n",
        "\n",
        "# Define the control point, vessel radius, and deployment site from the parameters\n",
        "control_point = np.array([parameter_vector[1], 0, parameter_vector[2]], dtype=np.float64)\n",
        "vessel_radius = parameter_vector[0]\n",
        "deployment_site = parameter_vector[3]\n",
        "\n",
        "# Update the vessel display with the new control point and radius\n",
        "visualizer.add_vessel(control_point, vessel_radius, deployment_site)\n",
        "visualizer.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJQqCbcrOi_Z"
      },
      "source": [
        "We can now interactively visualize the effect of the vessel parameters not only on the vessel shape itself, but also on how it affects the deployed stent configuration. Notice how fast this prediction is compared to a full finite element simulation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dcl6Lq8AFmdd"
      },
      "outputs": [],
      "source": [
        "# Interactively play with the parameters of the vessel and visualize the predicted stent configuration\n",
        "viz = utils.Visualizer()\n",
        "viz.add_vessel()\n",
        "viz.add_stent_from_file(\"Input/pos_stent.txt\", \"Input/conn_stent.txt\")\n",
        "\n",
        "def update_vessel_and_stent(x, z, r, s):\n",
        "    control_point = np.array([x, 0, z], dtype=np.float64)\n",
        "    viz.update_vessel(control_point, r, s)\n",
        "    # Predict deformed stent position\n",
        "    X_scaled = scaler_x.transform(np.array([[r, x, z, s]]))\n",
        "    y_pred_scaled_reduced = gp.predict(X_scaled)\n",
        "    y_pred = pca_simulations.inverse_transform(y_pred_scaled_reduced)\n",
        "    y_pred = scaler_y.inverse_transform(y_pred)\n",
        "    displacement_pred = np.array(y_pred[0]).reshape(-1, 3)\n",
        "    final_position_pred = position + displacement_pred\n",
        "    viz.update_stent(final_position_pred)\n",
        "\n",
        "    viz.show()\n",
        "\n",
        "w = interactive(update_vessel_and_stent, x=(control_point_x_bounds[0], control_point_x_bounds[1], 0.1), z=(control_point_z_bounds[0], control_point_z_bounds[1], 0.1), r=(vessel_radius_bounds[0], vessel_radius_bounds[1], 0.1), s=(deploy_site_bounds[0], deploy_site_bounds[1], 0.01))\n",
        "for c in w.children:\n",
        "    c.continuous_update = False\n",
        "display(w)"
      ]
    }
  ]
}