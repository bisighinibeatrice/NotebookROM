{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "5AVPRH4yXcLl",
        "ztqqLpCK00XI"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Case study: Predict stent deployed configuration in real time**\n",
        "\n",
        "In this class, we will build a data-driven, static reduced order model (ROM) of stent deployment capable of predicting in **real time** the stent deployed configuration given a certain vessel geometry."
      ],
      "metadata": {
        "id": "HQX1pGqVyeft"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **What are stents?**\n",
        "Stents are small, tube-like devices used to hold open narrowed or blocked arteries or other blood vessels in the body. They are often made of metal or plastic mesh and are commonly used in procedures like angioplasty to restore blood flow in cases of coronary artery disease. Some stents are coated with medication to prevent the artery from narrowing again, while others are simple mechanical supports.\n",
        "\n",
        "There exist two main types of stents. Balloon-expandable stents are made from materials like stainless steel and are deployed by inflating a balloon inside the stent, which forces it to expand. They provide high radial force and are ideal for precise placement, such as in coronary arteries. In contrast, self-expandable stents are made from shape-memory materials like nitinol. They expand on their own after being released from a catheter and are more flexible, making them suitable for areas that experience movement or compression, like peripheral arteries.\n",
        "\n",
        "In this class, we will focus on the self-expandable stents.\n",
        "\n",
        "![Balloon-expandable stent](https://i.imgur.com/dWsdVgo.jpeg)\n",
        "![Self-expandable stent](https://i.imgur.com/3d7OVob.jpeg)"
      ],
      "metadata": {
        "id": "vLvSGQjjyjCa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Why we need a ROM of stents?**\n",
        "Examples of why a ROM for stent deployment is needed:\n",
        "*  **Optimization Study:** A ROM enables running multiple simulations with varying geometrical or material properties of the stent to find the combination that maximizes specific performance metrics, such as radial force, flexibility, and fatigue resistance.\n",
        "*   **Planning Software:** By using a ROM, clinicians can compare different treatment options before actual deployment, evaluating various devices, sizes, and deployment sites to choose the most suitable option for each patient.\n",
        "*   **Intraoperative Support:** ROM can help visualize the deployed configuration overlaid on fluoroscopy images in real-time during the intervention, assisting surgeons with precise positioning and adjustments."
      ],
      "metadata": {
        "id": "-ejSuKhUyocA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **What is the focus of this class?**\n",
        "In this class, we will focus on building a ROM of the deployed configuration of a stent given a set of parameters describing the vessel geometry. In the first part of the class, we will work on an idealised vessel geometry. Our parameters will represent the curvature and radius of this vessel as long as the clinical decision on where to position the stent.\n",
        "\n",
        "![Parametric vessel](https://i.imgur.com/Em7mFE5.png)\n",
        "\n",
        "In the second part of the class, we will investigate how this model can be translated to a realistic case with patient-specific geeometries.\n",
        "\n",
        "![Patient-specific vessel](https://i.imgur.com/cdbmj58.png)\n",
        "\n"
      ],
      "metadata": {
        "id": "GWsXc18IuGrO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **How is the class structured?**\n",
        "1. Mathematical definition of the problem\n",
        "2. Sampling\n",
        "3. Building the database\n",
        "4. Building and testing the ROM\n",
        "\n",
        "![Schema](https://i.imgur.com/cjbySFE.png)"
      ],
      "metadata": {
        "id": "7lt8uMLJyuMR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Reference**\n",
        "About stents:\n",
        "\n",
        "*   https://www.sciencedirect.com/science/article/pii/S1350453310002420\n",
        "*   https://www.sciencedirect.com/science/article/pii/S0021929012003454\n",
        "*   https://www.sciencedirect.com/science/article/pii/S0021929012001789via%3Dihub\n",
        "\n",
        "Review on stents:\n",
        "*   https://www.sciencedirect.com/science/article/pii/S2666522023000175\n",
        "*   https://onlinelibrary.wiley.com/doi/full/10.1002/cnm.3529\n",
        "\n",
        "About reduced order modelling:\n",
        "\n",
        "* https://hal.science/hal-01007232/document\n",
        "* https://amses-journal.springeropen.com/articles/10.1186/s40323-015-0049-1\n",
        "* https://www.degruyter.com/document/doi/10.1515/9783110499001-008/html\n",
        "\n",
        "\n",
        "About this class:\n",
        "* https://www.frontiersin.org/journals/physiology/articles/10.3389/fphys.2023.1148540/full\n",
        "* https://www.sciencedirect.com/science/article/pii/S0045782518303487?casa_token=m-QUUFRG63IAAAAA:bDQR4oqhKto5bPIMnnyrbkdYk88o61SO0xvOC6Rc_8P8O6cvFljDK479s6fRatoobbGr2B2msA\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "I3W_DBcFywLs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Setting up the environment**\n"
      ],
      "metadata": {
        "id": "qmOMYsShkW5D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting up the environment\n",
        "import os\n",
        "os.chdir('/content')\n",
        "\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "# Check if the folder 'NotebookROM' exists before attempting to delete it to reinstall it\n",
        "if os.path.exists('NotebookROM'):\n",
        "    # Remove the folder and all its contents\n",
        "    shutil.rmtree('NotebookROM')\n",
        "\n",
        "# Pull the repo with data and install packages for the notebook if we're in Google Colab\n",
        "import os\n",
        "try:\n",
        "  import google.colab # Check if we're in Google Colab\n",
        "  !sudo apt install libgl1-mesa-glx xvfb\n",
        "  !pip install pyvista[jupyter] -qq\n",
        "  current_directory = os.path.basename(os.getcwd())\n",
        "  # If we're not in the right directory or the directory doesn't exist, clone the repo\n",
        "  if not (os.path.isdir('NotebookROM') or current_directory == 'NotebookROM'):\n",
        "    !git clone https://github.com/bisighinibeatrice/NotebookROM.git\n",
        "  if os.path.isdir('NotebookROM'):\n",
        "    %cd NotebookROM\n",
        "except:\n",
        "    pass"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Kll6SyVvVnZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the functions\n",
        "from importlib import reload\n",
        "import utils\n",
        "utils = reload(utils) # Reload the module in case it has changed\n",
        "import numpy as np\n",
        "from ipywidgets import interactive"
      ],
      "metadata": {
        "id": "fJ24c7y-6po3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Mathematical and physical definition of the problem**\n",
        "\n",
        "The problem we are modelling consists of two entities: the stent and the vessel. A helper class has been implemented to visualize a stent and the vessel. Here are the documentation and examples of how to use it."
      ],
      "metadata": {
        "id": "5AVPRH4yXcLl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "help(utils.Visualizer)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "f5iuWhdVaAMn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **stent** we are modeling is a braided design, consisting of 24 interlaced wires with a relaxed radius of 2.7 mm and a length of 12 mm. It is represented using **beam elements**. The mesh data, including node positions and connectivity, is saved in the `Data` folder. You can visualize the stent using the functions provided by the `Visualizer` object."
      ],
      "metadata": {
        "id": "4wkmffqebmsR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a visualizer object and add a stent to it\n",
        "viz = utils.Visualizer()\n",
        "viz.add_stent_from_file(\"Data/pos_stent.txt\", \"Data/conn_stent.txt\")\n",
        "viz.show()"
      ],
      "metadata": {
        "id": "BROL-r1UbmJ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**EXERCISE**: How many nodes compose the stent? How many elements?"
      ],
      "metadata": {
        "id": "V4MPR5TNgoPg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_name = \"Data/pos_stent.txt\"\n",
        "\n",
        "# Read the contents of the file\n",
        "with open(file_name, 'r') as file:\n",
        "    content = file.read()\n",
        "    # Split the content into lines and count them\n",
        "    num_lines = len(content.splitlines())\n",
        "\n",
        "# Print the number of lines\n",
        "print(f\"Number of nodes: {num_lines}\")\n",
        "\n",
        "file_name = \"Data/conn_stent.txt\"\n",
        "\n",
        "# Read the contents of the file\n",
        "with open(file_name, 'r') as file:\n",
        "    content = file.read()\n",
        "    # Split the content into lines and count them\n",
        "    num_lines = len(content.splitlines())\n",
        "\n",
        "# Print the number of lines\n",
        "print(f\"Number of elements: {num_lines}\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "TqMoE-kognol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **vessel** geometry is represented through a parameterized model, where the parameters serve as inputs for our ROM. The vessel's shape is defined by its **radius** and its **centerline**. With fixed starting and ending points, the centerline is parameterized using a third point, referred to as the control point, which is adjusted within the x-z plane.\n",
        "\n",
        "To create a smooth curve that connects these points, we employ Bernstein basis polynomials to weight the contributions of the three points—start point, control point, and endpoint. This approach effectively blends the points, resulting in a continuous and visually appealing path. This technique is widely used in computer graphics and computational geometry for generating smooth paths and shapes. The `spline_interpolation` function in the `utils` module implements this process, computing an interpolated point along a cubic Bézier curve defined by the three points.\n",
        "\n",
        "An additional parameter is considered in our analysis: the **deploy site** of the stent, which indicates its position at the beginning of deployment. This parameter ranges from 0 to 1, where 0 corresponds to the stent being positioned at the start of the vessel (proximal position) and 1 indicates that the stent is located at the end of the vessel (distal position).\n",
        "\n",
        "![Parametric vessel](https://i.imgur.com/dx4kMDu.png)"
      ],
      "metadata": {
        "id": "Kui1_avRcR9j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can visualize a generic vessel using the functions provided by the `Visualizer` object. The centerline is shown in black, the control point in red, and the deploy site in green."
      ],
      "metadata": {
        "id": "yPgEr6_tzlEc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a visualizer object and add a vessel to it\n",
        "viz = utils.Visualizer()\n",
        "viz.add_vessel()\n",
        "viz.show()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "sGhVQ8YJcRam"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Update the control point and radius\n",
        "control_point = np.array([60, 0, 25], dtype=np.float64)\n",
        "vessel_radius = 2\n",
        "deploy_site = 0.5\n",
        "\n",
        "# Update the vessel display\n",
        "viz.update_vessel(control_point, vessel_radius, deploy_site)\n",
        "viz.show()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "T05Ymfvacdmk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Interactively play with the parameters of the vessel\n",
        "viz = utils.Visualizer()\n",
        "viz.add_vessel()\n",
        "\n",
        "def update_vessel(x, z, r, s):\n",
        "    control_point = np.array([x, 0, z], dtype=np.float64)\n",
        "    viz.update_vessel(control_point, r, s)\n",
        "    viz.show()\n",
        "\n",
        "w = interactive(update_vessel, x=(-50, 50, 1), z=(0, 50, 1), r=(1, 20, 1), s=(0, 1, 0.1))\n",
        "for c in w.children:\n",
        "    c.continuous_update = False\n",
        "display(w)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Pq-kgzlbceC7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Sampling**\n",
        "\n",
        "In reduced order modelling, the sampling step is crucial for generating a representative set of data points that capture the variability of the system.The Latin Hypercube Sampling (LHS) technique is commonly used for this purpose. LHS is a statistical method that divides the parameter space into equally probable intervals and ensures that samples are distributed uniformly across the entire space. This technique reduces the risk of clustering and improves the efficiency of sampling compared to simple random sampling, making it ideal for high-dimensional problems. It helps in covering the parameter space more thoroughly with fewer samples, which enhances the accuracy of the ROM while minimizing computational costs."
      ],
      "metadata": {
        "id": "ztqqLpCK00XI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**EXERCISE:** This is a Python example of the use of LHS for sampling two parameters (`num_dimensions`). Plot the distribution of the points for different number of samples (`num_samples`)."
      ],
      "metadata": {
        "id": "WKGHaMdLlmS0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.stats import qmc\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define number of parameters\n",
        "num_dimensions = 2\n",
        "\n",
        "# Define sample size\n",
        "num_samples = 100\n",
        "\n",
        "# Create a Latin Hypercube Sampler\n",
        "lhs_sampler = qmc.LatinHypercube(d=num_dimensions)\n",
        "lhs_sample = lhs_sampler.random(n=num_samples)\n",
        "\n",
        "# Plot the results\n",
        "plt.scatter(lhs_sample[:, 0], lhs_sample[:, 1], c='green', label='Samples')\n",
        "plt.title('2D Latin Hypercube Sampling')\n",
        "plt.xlabel('Parameter 1')\n",
        "plt.ylabel('Parameter 2')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "i3EySgbG1YFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The LHS provides values between 0 and 1. How can we change the ranges in which the parameters are varying? We need to rescale the data.\n",
        "\n",
        "To scale the data from the unit interval [0,1] to a specified range [a,b], we can use this equation:\n",
        "\n",
        "$x_{\\text{scaled}} = a + (b - a) \\cdot x_{\\text{sample}}$\n",
        "\n",
        "where $x_{\\text{scaled}}$ is the scaled value, $x_{\\text{sample}}$ is the original LHS sample in the range [0,1], $a$ is the lower bound of the target range, and $b$ is the upper bound of the target range.\n",
        "\n",
        "**EXERCISE:** Implement this equation to rescale the data and plot the scaled results."
      ],
      "metadata": {
        "id": "-K_k3v4Qlwbp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters bounds\n",
        "param1_bounds = (0.0, 1.0)  # Range for Parameter 1\n",
        "param2_bounds = (0.0, 5.0)  # Range for Parameter 2\n",
        "\n",
        "# Updated bounds for the two parameters\n",
        "bounds = [param1_bounds, param2_bounds]\n",
        "\n",
        "# Scale the LHS sample to the given bounds\n",
        "scaled_lhs_sample = np.empty_like(lhs_sample)\n",
        "for i, (lower, upper) in enumerate(bounds):\n",
        "    scaled_lhs_sample[:, i] = lower + lhs_sample[:, i] * (upper - lower)\n",
        "\n",
        "# Plot the results\n",
        "plt.scatter(scaled_lhs_sample[:, 0], scaled_lhs_sample[:, 1], c='red', label='Samples')\n",
        "plt.title('2D Latin Hypercube Sampling')\n",
        "plt.xlabel('Parameter 1 (Range: 0 to 1)')\n",
        "plt.ylabel('Parameter 2 (Range: 0 to 5)')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kowjpuO0lwHU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's go back to our original problem of interest. We have 4 parameters (vessel radius, control point x and y coordinates, and deploy site), which are varying in these ranges:\n",
        "*   `vessel_radius_bounds = (1.0, 2.5)`\n",
        "*   `control_point_x_bounds = (0.0, 50.0)`\n",
        "*   `control_point_y_bounds = (0.0, 50.0)`\n",
        "*   `deploy_site_bounds = (0.01, 1.0)`\n",
        "\n",
        "**EXERCISE:**  Change the LHS code to compute `num_samples = 5` with LHS (considering the ranges indicated above) and visulise them."
      ],
      "metadata": {
        "id": "H04VNLPo1hpg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "num_dimensions = 4\n",
        "vessel_radius_bounds = (1.0, 2.5)  # Range for vessel radius\n",
        "control_point_x_bounds = (0.0, 50.0)  # Control point X bounds\n",
        "control_point_y_bounds = (0.0, 50.0)  # Control point Y bounds\n",
        "deploy_site_bounds = (0.01, 1.0)  # Range for deploy_site (0 to 1)\n",
        "\n",
        "# Updated bounds to include deploy_site\n",
        "bounds = [vessel_radius_bounds, control_point_x_bounds, control_point_y_bounds, deploy_site_bounds]\n",
        "\n",
        "# Define sample size\n",
        "num_samples = 4\n",
        "\n",
        "# Create a Latin Hypercube Sampler\n",
        "lhs_sampler = qmc.LatinHypercube(d=num_dimensions)\n",
        "lhs_sample = lhs_sampler.random(n=num_samples)\n",
        "\n",
        "# Scale the LHS sample to the given bounds\n",
        "scaled_lhs_sample = np.empty_like(lhs_sample)\n",
        "for i, (lower, upper) in enumerate(bounds):\n",
        "  scaled_lhs_sample[:, i] = lower + lhs_sample[:, i] * (upper - lower)\n",
        "\n",
        "# Output the train subsets and the test subset\n",
        "np.savetxt(f\"parameters_{num_samples}.txt\", scaled_lhs_sample, fmt=\"%.2f\")\n",
        "\n",
        "# Visualize the vessels represented by all the samples\n",
        "viz = utils.Visualizer()\n",
        "for i in range(num_samples):\n",
        "  print(f\"Sample {i+1}: {scaled_lhs_sample[i]}\")\n",
        "  viz.add_vessel(control_point=np.array([scaled_lhs_sample[i][1], 0., scaled_lhs_sample[i][2]]), vessel_radius=scaled_lhs_sample[i][0], vessel_opacity=0.1)\n",
        "viz.show()"
      ],
      "metadata": {
        "id": "Dru5laci1hKP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. Building the database**\n",
        "\n",
        "To create the ROM, we considered four scenarios with an increasing number of cases: 100, 200, 500, and 1000. Using the LHS method, we determined the parameter values and generated the corresponding geometries. For each of these geometries, we then ran a simulation of stent deployment using an in-house, finite-element software. This simulation consists in computing, starting from the stent configuration previously shown and the deploy site defined by the LHS, the deployed configuration of the stent."
      ],
      "metadata": {
        "id": "U0UsyCpxMGKx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "These simulations are performed in three steps:\n",
        "\n",
        "1. First the stent is crimped. ![Crimping](https://i.imgur.com/xSBHAJf.gif)\n",
        "\n",
        "2. Then, positioned along the centerline of the vessel. ![Positioning](https://i.imgur.com/ChGbkJU.gif)\n",
        "\n",
        "3. And finally, deployed. ![Deployment](https://i.imgur.com/hsWHPZJ.gif)\n",
        "\n",
        "As this topic will not be covered in the current class, additional information on finite-element simulations of stent deploymeet can be found in the literature provided as reference material for interested readers."
      ],
      "metadata": {
        "id": "3jkvOcPs3spm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the following step, the data corresponding to the input (`parameters_$N.txt`) and output (`simulations_$N.txt`) of these simulations was saved in the `Data` folder. The output consists in the displacements of the stent nodes at the end of the simulation. We can visualise a sample case using the `Visualizer` tool."
      ],
      "metadata": {
        "id": "oYSMevX1qbFP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the database size and the case\n",
        "N = 100\n",
        "line_number = 50  # Line number to be read from the displacements file\n",
        "\n",
        "# Specify the names of the input files\n",
        "position_file = \"Data/pos_stent.txt\"  # Position data file (positions for the stent)\n",
        "connectivity_file = \"Data/conn_stent.txt\"  # Connectivity data file\n",
        "displacement_file = f\"Data/simulations_{N}.txt\"  # Displacement data file (displacement vector for simulations)\n",
        "\n",
        "# Read the position matrix from position_file\n",
        "position = np.loadtxt(position_file)\n",
        "connectivity = np.loadtxt(connectivity_file).astype(np.int32)\n",
        "\n",
        "# Open displacement_file to read the specified line (line_number)\n",
        "with open(displacement_file, 'r') as displacement_file_handle:\n",
        "    # Read all lines into a list\n",
        "    lines = displacement_file_handle.readlines()\n",
        "    if line_number <= len(lines):  # Ensure that line_number is within the number of available lines\n",
        "        displacement_vector = list(map(float, lines[line_number - 1].strip().split()))  # Read and convert the line to floats\n",
        "    else:\n",
        "        raise ValueError(f\"Line {line_number} does not exist in the file.\")\n",
        "\n",
        "# Reshape the displacement vector into a matrix with 3 columns\n",
        "displacement = np.array(displacement_vector).reshape(-1, 3)\n",
        "\n",
        "# Add the position matrix and displacement matrix element-wise to get the final position matrix\n",
        "final_position = position + displacement\n",
        "\n",
        "# Create a visualizer object and add a stent to it\n",
        "visualizer = utils.Visualizer()\n",
        "visualizer.add_stent(final_position, connectivity)\n",
        "\n",
        "# Update the control point and radius based on parameters from a different file\n",
        "parameter_file = f\"Data/parameters_{N}.txt\"\n",
        "\n",
        "with open(parameter_file, 'r') as parameter_file_handle:\n",
        "    # Read all lines from the parameter file\n",
        "    lines = parameter_file_handle.readlines()\n",
        "    if line_number <= len(lines):  # Ensure that line_number is within the number of available lines\n",
        "        parameter_vector = list(map(float, lines[line_number - 1].strip().split()))  # Read and convert the line to floats\n",
        "    else:\n",
        "        raise ValueError(f\"Line {line_number} does not exist in the file.\")\n",
        "\n",
        "# Define the control point, vessel radius, and deployment site from the parameters\n",
        "control_point = np.array([parameter_vector[1], 0, parameter_vector[2]], dtype=np.float64)\n",
        "vessel_radius = parameter_vector[0]\n",
        "deployment_site = parameter_vector[3]\n",
        "\n",
        "# Update the vessel display with the new control point and radius\n",
        "visualizer.add_vessel(control_point, vessel_radius, deployment_site)\n",
        "visualizer.show()"
      ],
      "metadata": {
        "id": "rXpnKZiNy60q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4. Building the ROM**\n",
        "\n",
        "Machine learning is a key technique used in building ROMs for complex simulations. In this class, we will focus on regression models. **Regression** involves fitting a mathematical function to approximate the relationship between input parameters (e.g., geometry, material properties) and output variables (e.g., stress, displacement). There are many machine learning techniques, ranging from simpler models like linear regression to more complex models capable of capturing nonlinear relationships, such as neural networks.\n",
        "\n",
        "In this class, we will use Gaussian Process regression (GPR), as it is well-suited for small training datasets and has previously been applied to nonlinear structural problems."
      ],
      "metadata": {
        "id": "vLPZ1L9JTAC6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from sklearn.gaussian_process.kernels import RBF, WhiteKernel\n",
        "from sklearn.gaussian_process.kernels import ConstantKernel as C\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "# Load parameters and simulations from text files\n",
        "def load_data(parameters_file, simulations_file):\n",
        "    parameters = np.loadtxt(parameters_file)\n",
        "    simulations = np.loadtxt(simulations_file)\n",
        "    return parameters, simulations"
      ],
      "metadata": {
        "id": "xNCfzwz-UJp_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initially, we will start from the smallest database (number of cases: `N = 100`) and use the data in its raw form as input for the machine learning model. The data has to be split between training set (used to train the model) and testing set (used to test the model, not used during the training)."
      ],
      "metadata": {
        "id": "A3N5nute05x5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data\n",
        "N = 100\n",
        "parameters_file = f\"Data/parameters_{N}.txt\"  # Replace with your actual file name\n",
        "simulations_file = f\"Data/simulations_{N}.txt\"  # Replace with your actual file name\n",
        "parameters, simulations = load_data(parameters_file, simulations_file)\n",
        "indices = np.arange(len(parameters))\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test, idx_train, idx_test = train_test_split(parameters, simulations, indices, test_size=0.25, random_state=42)"
      ],
      "metadata": {
        "id": "MH243fWMd6xf",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Gaussian Process Regression, the choice of kernel function defines the behavior of the model. It defines how similar points influence each other in predictions. Here, we’ll use a common choice: a combination of the RBF kernel and a constant kernel."
      ],
      "metadata": {
        "id": "5-ImtIhQ1GHM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the Kriging model on the data: define a kernel for the Gaussian Process\n",
        "initial_kernel = C(1.0, (1e0, 1e3)) * RBF(1.0, (1e-2, 1e2))"
      ],
      "metadata": {
        "id": "v_U-w1YF1H0E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, we can create the GPR model and fit the data."
      ],
      "metadata": {
        "id": "nKAqfKVb1TXM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the GaussianProcessRegressor with the defined kernel\n",
        "gp = GaussianProcessRegressor(kernel=initial_kernel, n_restarts_optimizer=10, random_state=42)\n",
        "\n",
        "# Fit the Gaussian Process model to the reduced data\n",
        "gp.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "FtTB73L21S2F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that it has been trained, we can use it to meke prediction."
      ],
      "metadata": {
        "id": "8UgJDaEJ1aLM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict using the Kriging model on the test data\n",
        "y_pred = gp.predict(X_test)"
      ],
      "metadata": {
        "id": "itMStIJn2gp5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model has been successfully trained, and now we need to **evaluate** its performance. For a regression model, we can assess its performance using metrics such as:\n",
        "*   Mean squared error (MSE);\n",
        "*   Mean absolute error (MAE).\n",
        "\n",
        "MSE measures the average squared difference between predicted and actual values, providing insight into the model's accuracy, while MAE gives a straightforward average of absolute errors, making it easier to interpret."
      ],
      "metadata": {
        "id": "gDzDlcSWI9GR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate MSE and MAAE for each case\n",
        "mae_vec = []\n",
        "rmse_vec = []\n",
        "for i in range(y_test.shape[0]):\n",
        "    mae = mean_absolute_error(y_test[i, :], y_pred[i, :])\n",
        "    rmse = np.sqrt(mean_squared_error(y_test[i, :], y_pred[i, :]))\n",
        "    mae_vec.append(mae)\n",
        "    rmse_vec.append(rmse)\n",
        "\n",
        "print(f\" MAE: {np.mean(mae_vec)}\")\n",
        "print(f\" RMSE: {np.mean(rmse_vec)} mm\")"
      ],
      "metadata": {
        "id": "MY88wF1pnsEP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we observe, the errors are quite high, so the machine learning model struggles to establish a mapping between the input and output when using the raw data.\n",
        "\n",
        "This could be due to the large variation between the input and output data. To evaluate this, we can compute statistical measures like the variance or standard deviation for both the input and output datasets. These metrics will help us quantify the spread or dispersion in the data, providing insight into whether significant differences in scale or range between the input and output are affecting the model's ability to learn."
      ],
      "metadata": {
        "id": "DkFfkNhaeObm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate standard deviationfor input and output\n",
        "input_std_dev = np.std(parameters, axis=0)\n",
        "output_std_dev = np.std(simulations, axis=0)\n",
        "\n",
        "print(input_std_dev)\n",
        "print(output_std_dev)"
      ],
      "metadata": {
        "id": "RNQDRx7lqdcM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To improve performance, we can **standardize** the data, which involves transforming the features to have a mean of zero and a standard deviation of one."
      ],
      "metadata": {
        "id": "Y8eKbn40os_2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Standardize both training and test sets for the output\n",
        "scaler_y = StandardScaler()\n",
        "y_train_scaled = scaler_y.fit_transform(y_train)\n",
        "y_test_scaled = scaler_y.transform(y_test)\n",
        "\n",
        "# Standardize both training and test sets for the input\n",
        "scaler_x = StandardScaler()\n",
        "X_train_scaled = scaler_x.fit_transform(X_train)\n",
        "X_test_scaled = scaler_x.transform(X_test)"
      ],
      "metadata": {
        "id": "4Uwm_J-WlaXx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's visualise the change in variance."
      ],
      "metadata": {
        "id": "K26HNsNb3Muu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the mean values for the first 10 output variables before and after scaling\n",
        "mean_before = np.mean(y_train[:, :10], axis=0)\n",
        "mean_after = np.mean(y_train_scaled[:, :10], axis=0)\n",
        "\n",
        "# Compute the standard deviations for the first 10 output variables before and after scaling\n",
        "std_before = np.std(y_train[:, :10], axis=0)\n",
        "std_after = np.std(y_train_scaled[:, :10], axis=0)\n",
        "\n",
        "# Plot the means with error bars representing standard deviation before scaling\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.errorbar(range(1, 11), mean_before, yerr=std_before, fmt='o', label='Before Scaling', color='blue', capsize=5)\n",
        "plt.title('Mean and Standard Deviation for the First 10 Output Variables (Before Scaling)')\n",
        "plt.xlabel('Output Variable')\n",
        "plt.ylabel('Mean Value')\n",
        "plt.xticks(range(1, 11))\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Plot the means with error bars representing standard deviation after scaling\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.errorbar(range(1, 11), mean_after, yerr=std_after, fmt='o', label='After Scaling', color='orange', capsize=5)\n",
        "plt.title('Mean and Standard Deviation for the First 10 Output Variables (After Scaling)')\n",
        "plt.xlabel('Output Variable')\n",
        "plt.ylabel('Mean Value')\n",
        "plt.xticks(range(1, 11))\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "doni-hmf20Fi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's retrain the machine learning model but with the scaled data now."
      ],
      "metadata": {
        "id": "J7_sGNy53Hd6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the Kriging model on the data: define a kernel for the Gaussian Process\n",
        "initial_kernel = C(1.0, (1e-1, 1e3)) * RBF(1.0, (1e-2, 1e2))\n",
        "\n",
        "# Create the GaussianProcessRegressor with the defined kernel\n",
        "gp = GaussianProcessRegressor(kernel=initial_kernel, n_restarts_optimizer=10, random_state=42)\n",
        "\n",
        "# Measure the time taken to fit the model\n",
        "start_time = time.time()\n",
        "gp.fit(X_train_scaled, y_train_scaled)\n",
        "end_time = time.time()\n",
        "\n",
        "# Compute the computational time\n",
        "fit_time = end_time - start_time\n",
        "print(f\"Time taken to fit the Gaussian Process model: {fit_time:.4f} seconds\")\n",
        "\n",
        "# Predict using the Kriging model on the test data\n",
        "y_pred_scaled = gp.predict(X_test_scaled)\n",
        "\n",
        "# Inverse transform to reconstruct the original output\n",
        "y_pred = scaler_y.inverse_transform(y_pred_scaled)\n",
        "\n",
        "# Calculate MAE for each case\n",
        "mae_vec = []\n",
        "rmse_vec = []\n",
        "for i in range(y_test.shape[0]):\n",
        "    mae = mean_absolute_error(y_test[i, :], y_pred[i, :])\n",
        "    rmse = np.sqrt(mean_squared_error(y_test[i, :], y_pred[i, :]))\n",
        "    mae_vec.append(mae)\n",
        "    rmse_vec.append(rmse)\n",
        "\n",
        "print(f\" MAE: {np.mean(mae_vec)}\")\n",
        "print(f\" RMSE: {np.mean(rmse_vec)} mm\")"
      ],
      "metadata": {
        "id": "w9mEbiJR2Bmv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code still takes a lot of time to fit the machine learning model, which may be due to the complexity of learning all of these outputs simultaneously.\n",
        "\n",
        "**EXERCISE:** How many output is our system composed of?"
      ],
      "metadata": {
        "id": "5E_ghUmRlpew"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_train.shape)"
      ],
      "metadata": {
        "id": "1z33VTWDq_D9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To reduce the training time, we can **compress** this output, and one effective approach is to utilize Principal Component Analysis (PCA). This technique reduces the dimensionality of the data by transforming it into a set of uncorrelated variables, known as principal components, which capture the most significant variance in the output while minimizing information loss."
      ],
      "metadata": {
        "id": "gs5LTCqlmA4b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the PCA on the output (simulations)\n",
        "n_components = 25\n",
        "pca_simulations = PCA(n_components=n_components)\n",
        "\n",
        "# Fit and transform the output\n",
        "y_train_scaled_reduced = pca_simulations.fit_transform(y_train_scaled)\n",
        "\n",
        "# Plot the cumulative explained variance\n",
        "explained_variance = np.cumsum(pca_simulations.explained_variance_ratio_)\n",
        "\n",
        "# Create the plot\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(range(1, n_components + 1), explained_variance, marker='o', linestyle='--', color='b')\n",
        "plt.xlabel('Number of Components')\n",
        "plt.ylabel('Cumulative Explained Variance')\n",
        "plt.title('PCA Convergence: Cumulative Explained Variance vs. Number of Components')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VY9W9wINm4so"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This plot shows the cumulative explained variance against the number of principal components in a PCA analysis:\n",
        "\n",
        "* **X-axis**: Number of principal components added.\n",
        "* **Y-axis**: Cumulative explained variance—how much of the data’s variance is captured as more components are included.\n",
        "* **Curve**: The curve rises quickly at first, indicating that the initial components capture most of the variance, then levels off, suggesting diminishing returns from additional components.\n",
        "\n",
        "This plot helps identify the \"elbow point,\" where adding more components provides minimal additional explained variance, guiding you to select an optimal number of components for dimensionality reduction."
      ],
      "metadata": {
        "id": "TU7pfdda8KvU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's rebuild the GPR model with the reduced data."
      ],
      "metadata": {
        "id": "vzrjcquh8CzS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the Kriging model on the data: define a kernel for the Gaussian Process\n",
        "initial_kernel = C(1.0, (1e2, 1e5)) * RBF(1.0, (1e-2, 1e2)) + WhiteKernel(noise_level=1e-2)\n",
        "\n",
        "# Create the GaussianProcessRegressor with the defined kernel\n",
        "gp = GaussianProcessRegressor(kernel=initial_kernel, n_restarts_optimizer=10, random_state=42)\n",
        "\n",
        "# Measure the time taken to fit the model\n",
        "start_time = time.time()\n",
        "gp.fit(X_train_scaled, y_train_scaled_reduced)\n",
        "end_time = time.time()\n",
        "\n",
        "# Compute the computational time\n",
        "fit_time = end_time - start_time\n",
        "print(f\"Time taken to fit the Gaussian Process model: {fit_time:.4f} seconds\")\n",
        "\n",
        "# Predict using the Kriging model on the test data\n",
        "y_pred_scaled_reduced = gp.predict(X_test_scaled)\n",
        "\n",
        "# Inverse transform to reconstruct the original output\n",
        "y_pred_scaled = pca_simulations.inverse_transform(y_pred_scaled_reduced)\n",
        "y_pred = scaler_y.inverse_transform(y_pred_scaled)\n",
        "\n",
        "# Calculate MAE for each case\n",
        "mae_vec = []\n",
        "rmse_vec = []\n",
        "for i in range(y_test.shape[0]):\n",
        "    mae = mean_absolute_error(y_test[i, :], y_pred[i, :])\n",
        "    rmse = np.sqrt(mean_squared_error(y_test[i, :], y_pred[i, :]))\n",
        "    mae_vec.append(mae)\n",
        "    rmse_vec.append(rmse)\n",
        "\n",
        "print(f\" MAE: {np.mean(mae_vec)}\")\n",
        "print(f\" RMSE: {np.mean(rmse_vec)} mm\")"
      ],
      "metadata": {
        "id": "Ln3atvRv0OgF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add visualisation of stent predicted on top of simulated one."
      ],
      "metadata": {
        "id": "d99-QW5eSQ8-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the case\n",
        "N = 100\n",
        "case = 20  # Case number to use from y_pred\n",
        "\n",
        "# Specify the name of the position file\n",
        "position_file = \"Data/pos_stent.txt\"  # Position data file (positions for the stent)\n",
        "connectivity_file = \"Data/conn_stent.txt\"\n",
        "\n",
        "# Read the position matrix from position_file\n",
        "position = np.loadtxt(position_file)\n",
        "connectivity = np.loadtxt(connectivity_file).astype(np.int32)\n",
        "\n",
        "# Ensure that case is within the range of y_pred (predicted displacements)\n",
        "if case <= len(y_pred):  # Assuming y_pred is a list of lists or a 2D array where each row is a displacement vector\n",
        "    displacement_pred_vector = y_pred[case - 1]  # Select the specified case from y_pred (predicted displacement)\n",
        "else:\n",
        "    raise ValueError(f\"Case {case} does not exist in y_pred.\")\n",
        "\n",
        "# Reshape the displacement vector into a matrix with 3 columns\n",
        "displacement_pred = np.array(displacement_pred_vector).reshape(-1, 3)\n",
        "\n",
        "# Add the position matrix and predicted displacement matrix element-wise to get the predicted final position matrix\n",
        "final_position_pred = position + displacement_pred\n",
        "\n",
        "# Ensure that case is within the range of y_test (actual displacements)\n",
        "if case <= len(y_test):  # Assuming y_test is a list of lists or a 2D array where each row is a displacement vector\n",
        "    displacement_actual_vector = y_test[case - 1]  # Select the specified case from y_test (actual displacement)\n",
        "else:\n",
        "    raise ValueError(f\"Case {case} does not exist in y_test.\")\n",
        "\n",
        "# Reshape the displacement vector into a matrix with 3 columns\n",
        "displacement_actual = np.array(displacement_actual_vector).reshape(-1, 3)\n",
        "\n",
        "# Add the position matrix and actual displacement matrix element-wise to get the final position matrix\n",
        "final_position_actual = position + displacement_actual\n",
        "\n",
        "# Create a visualizer object and add both stents to it\n",
        "visualizer = utils.Visualizer()\n",
        "visualizer.add_stent(final_position_pred, connectivity, color=\"green\")  # Predicted stent in green\n",
        "visualizer.add_stent(final_position_actual, connectivity, color=\"red\")  # Actual stent in red\n",
        "\n",
        "# Update the control point and radius based on parameters from a different file\n",
        "parameter_file = f\"Data/parameters_{N}.txt\"\n",
        "\n",
        "with open(parameter_file, 'r') as parameter_file_handle:\n",
        "    # Read all lines from the parameter file\n",
        "    lines = parameter_file_handle.readlines()\n",
        "    if idx_test[case - 1] <= len(lines):  # Ensure that case is within the number of available lines\n",
        "        parameter_vector = list(map(float, lines[idx_test[case - 1]].strip().split()))  # Read and convert the line to floats\n",
        "    else:\n",
        "        raise ValueError(f\"Case {idx_test[case - 1]} does not exist in the parameter file.\")\n",
        "\n",
        "# Define the control point, vessel radius, and deployment site from the parameters\n",
        "control_point = np.array([parameter_vector[1], 0, parameter_vector[2]], dtype=np.float64)\n",
        "vessel_radius = parameter_vector[0]\n",
        "deployment_site = parameter_vector[3]\n",
        "\n",
        "# Update the vessel display with the new control point and radius\n",
        "visualizer.add_vessel(control_point, vessel_radius, deployment_site)\n",
        "visualizer.show()"
      ],
      "metadata": {
        "id": "BJC9TTAB9r1N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now compare the performance of the reduced order model when we use a larger training database."
      ],
      "metadata": {
        "id": "DxVBSv3OzjHs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare lists to store the errors\n",
        "N_values = [100, 250, 500, 1000]\n",
        "rmse_results = []\n",
        "rmse_std_results = []\n",
        "\n",
        "for N in N_values:\n",
        "\n",
        "    # Example usage\n",
        "    parameters_file = f\"Data/parameters_{N}.txt\"  # Replace with your actual file name\n",
        "    simulations_file = f\"Data/simulations_{N}.txt\"  # Replace with your actual file name\n",
        "\n",
        "    # Load the data\n",
        "    parameters, simulations = load_data(parameters_file, simulations_file)\n",
        "    indices = np.arange(len(parameters))\n",
        "\n",
        "    # Split the data into training and test sets\n",
        "    X_train, X_test, y_train, y_test, idx_train, idx_test = train_test_split(parameters, simulations, indices, test_size=0.25, random_state=42)\n",
        "\n",
        "    # Save the idx_test (indices of the test set) to a text file\n",
        "    idx_test_file = f\"Data/idx_test_{N}.txt\"\n",
        "    np.savetxt(idx_test_file, idx_test, fmt='%d', delimiter=\"\\t\")\n",
        "\n",
        "    # Standardize both training and test sets for the output\n",
        "    scaler_y = StandardScaler()\n",
        "    y_train_scaled = scaler_y.fit_transform(y_train)\n",
        "    y_test_scaled = scaler_y.transform(y_test)\n",
        "\n",
        "    # Standardize both training and test sets for the input\n",
        "    scaler_x = StandardScaler()\n",
        "    X_train_scaled = scaler_x.fit_transform(X_train)\n",
        "    X_test_scaled = scaler_x.transform(X_test)\n",
        "\n",
        "    # Create the PCA on the output (simulations)\n",
        "    n_components = 50\n",
        "    pca_simulations = PCA(n_components=n_components)\n",
        "\n",
        "    # Fit and transform the output\n",
        "    y_train_scaled_reduced = pca_simulations.fit_transform(y_train_scaled)\n",
        "\n",
        "    # Build the Kriging model on the data: define a kernel for the Gaussian Process\n",
        "    initial_kernel = C(1.0, (1e2, 1e5)) * RBF(1.0, (1e-2, 1e2)) + WhiteKernel(noise_level=1e-2)\n",
        "\n",
        "    # Create the GaussianProcessRegressor with the defined kernel\n",
        "    gp = GaussianProcessRegressor(kernel=initial_kernel, n_restarts_optimizer=10, random_state=42)\n",
        "\n",
        "    # Fit the Gaussian Process model to the reduced data\n",
        "    gp.fit(X_train_scaled, y_train_scaled_reduced)\n",
        "\n",
        "    # Predict using the Kriging model on the test data\n",
        "    y_pred_scaled_reduced = gp.predict(X_test_scaled)\n",
        "\n",
        "    # Inverse transform to reconstruct the original output\n",
        "    y_pred_scaled = pca_simulations.inverse_transform(y_pred_scaled_reduced)\n",
        "    y_pred = scaler_y.inverse_transform(y_pred_scaled)\n",
        "\n",
        "    # Save predictions to a text file\n",
        "    prediction_file = f\"Data/predictions_{N}.txt\"\n",
        "    np.savetxt(prediction_file, y_pred, fmt='%.6f', delimiter=\"\\t\")\n",
        "\n",
        "    # Calculate MAE for each case\n",
        "    mae_vec = []\n",
        "    rmse_vec = []\n",
        "    for i in range(y_test.shape[0]):\n",
        "        rmse = np.sqrt(mean_squared_error(y_test[i, :], y_pred[i, :]))\n",
        "        rmse_vec.append(rmse)\n",
        "\n",
        "    print(f\"Number of training cases: {N}\")\n",
        "    print(f\"  mean of RMSE: {np.mean(rmse_vec)} mm\")\n",
        "    print(f\"  median of RMSE: {np.median(rmse_vec)} mm \")\n",
        "    print(f\"  sd of RMSE: {np.std(rmse_vec)} mm\")\n",
        "\n",
        "    # Store the results\n",
        "    rmse_results.append(np.mean(rmse_vec))\n",
        "    rmse_std_results.append(np.std(rmse_vec))\n",
        "\n",
        "# Plotting MAE and RMSE vs. number of training samples\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.errorbar(N_values, mae_results, yerr=mae_std_results, label='Mean Absolute Error (MAE)', marker='o', capsize=5)\n",
        "plt.xlabel(\"Number of Training Samples\")\n",
        "plt.ylabel(\"Error\")\n",
        "plt.title(\"Error vs Number of Training Samples\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "RIjQDJnNzr2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, let's visualise the stent predited by training the machine learning model with `N = 1000` cases."
      ],
      "metadata": {
        "id": "LazdnmCWNfFn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the database and case\n",
        "N = 500\n",
        "case = 3  # Case number to use from y_pred and y_actual\n",
        "\n",
        "# Specify the name of the position file\n",
        "position_file = \"Data/pos_stent.txt\"  # Position data file (positions for the stent)\n",
        "connectivity_file = \"Data/conn_stent.txt\"\n",
        "\n",
        "# Read the position matrix from position_file\n",
        "position = np.loadtxt(position_file)\n",
        "connectivity = np.loadtxt(connectivity_file).astype(np.int32)\n",
        "\n",
        "# Read the predicted displacements (y_pred) from the prediction file (one line for each case)\n",
        "predictions_file = f\"Data/predictions_{N}.txt\"\n",
        "y_pred = np.loadtxt(predictions_file)\n",
        "\n",
        "# Ensure that case is within the range of y_pred (predicted displacements)\n",
        "if case <= len(y_pred):  # Assuming y_pred is a list of rows where each row is a displacement vector\n",
        "    displacement_pred_vector = y_pred[case - 1]  # Select the specified case (predicted displacement)\n",
        "else:\n",
        "    raise ValueError(f\"Case {case} does not exist in y_pred.\")\n",
        "\n",
        "# Reshape the displacement vector into a matrix with 3 columns\n",
        "displacement_pred = np.array(displacement_pred_vector).reshape(-1, 3)\n",
        "\n",
        "# Add the position matrix and predicted displacement matrix element-wise to get the predicted final position matrix\n",
        "final_position_pred = position + displacement_pred\n",
        "\n",
        "# Read the actual displacements (y_actual) from the simulations file (one line for each case)\n",
        "simulations_file = f\"Data/simulations_{N}.txt\"\n",
        "y_actual = np.loadtxt(simulations_file)\n",
        "\n",
        "# Read idx_test from the previously saved text file\n",
        "idx_test_file = f\"Data/idx_test_{N}.txt\"\n",
        "idx_test = np.loadtxt(idx_test_file, dtype=int)  # Read idx_test values from the file\n",
        "\n",
        "# Ensure that case is within the range of idx_test (test indices)\n",
        "if idx_test[case - 1] <= len(y_actual):  # Ensure the case is within the range of actual displacements\n",
        "    displacement_actual_vector = y_actual[idx_test[case - 1]]  # Select the specified case (actual displacement)\n",
        "else:\n",
        "    raise ValueError(f\"Case {idx_test[case - 1]} does not exist in y_actual.\")\n",
        "\n",
        "# Reshape the displacement vector into a matrix with 3 columns\n",
        "displacement_actual = np.array(displacement_actual_vector).reshape(-1, 3)\n",
        "\n",
        "# Add the position matrix and actual displacement matrix element-wise to get the final position matrix\n",
        "final_position_actual = position + displacement_actual\n",
        "\n",
        "# Create a visualizer object and add both stents to it\n",
        "visualizer = utils.Visualizer()\n",
        "visualizer.add_stent(final_position_pred, connectivity, color=\"green\")  # Predicted stent in green\n",
        "visualizer.add_stent(final_position_actual, connectivity, color=\"red\")  # Actual stent in red\n",
        "\n",
        "# Update the control point and radius based on parameters from a different file\n",
        "parameter_file = f\"Data/parameters_{N}.txt\"\n",
        "\n",
        "with open(parameter_file, 'r') as parameter_file_handle:\n",
        "    # Read all lines from the parameter file\n",
        "    lines = parameter_file_handle.readlines()\n",
        "    if idx_test[case - 1] <= len(lines):  # Ensure that case is within the number of available lines\n",
        "        parameter_vector = list(map(float, lines[idx_test[case - 1]].strip().split()))  # Read and convert the line to floats\n",
        "    else:\n",
        "       raise ValueError(f\"Case {idx_test[case - 1]} does not exist in the parameter file.\")\n",
        "\n",
        "# Define the control point, vessel radius, and deployment site from the parameters\n",
        "control_point = np.array([parameter_vector[1], 0, parameter_vector[2]], dtype=np.float64)\n",
        "vessel_radius = parameter_vector[0]\n",
        "deployment_site = parameter_vector[3]\n",
        "\n",
        "# Update the vessel display with the new control point and radius\n",
        "visualizer.add_vessel(control_point, vessel_radius, deployment_site)\n",
        "visualizer.show()"
      ],
      "metadata": {
        "id": "XSZVOra2G3Xv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}